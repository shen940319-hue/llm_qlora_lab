{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c85f24-e23e-4097-9541-bf5c11e7cea9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1ada4-ab4d-453b-8ae0-61caeb108c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install peft bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd17a7-770f-42c9-8bd2-315e782a3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set BitsAndBytesConfig to enable 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,        # Enable 4-bit quantization\n",
    "        bnb_4bit_quant_type=\"nf4\", # Choose the quantization type (commonly 'fp4' or 'nf4')\n",
    "        bnb_4bit_compute_dtype=torch.float16,# Set computation type to float16\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # Enable CPU FP32 offload\n",
    "\n",
    "    )\n",
    "    model_id = \"LLM-Research/Meta-Llama-3.1-70B-Instruct\"\n",
    "    #load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",  # Automatically allocate to available GPU\n",
    "        quantization_config=bnb_config,\n",
    "        resume_download=True  # Attempt checkpoint/resume\n",
    "\n",
    "    )\n",
    "    \n",
    "    # # Load the fine-tuned LoRA adapter (path where it was saved)\n",
    "    #model = PeftModel.from_pretrained(base_model, \"./llama3-70b-qlora-triples/checkpoint-347\")\n",
    "\n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Optional system prompt setting, adjust according to your needs\n",
    "    tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4a160-323f-4769-9926-2685bea55675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must be executed during inference; not required during fine-tuning\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35dda91-05aa-4da6-b4d4-f5142925524a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.chat_template = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "    \"{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] }}{% endif %}<|eot_id|>\"\n",
    "    \"{% for message in messages[1:] %}\"\n",
    "    \"<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n{{ message['content'] }}<|eot_id|>\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c257266-0c1f-45ea-8c1f-e4ba79d677bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4ff07-6df1-44ff-8278-7eb235e96389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12de52-3520-464c-a923-7c9fd9d36d2a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import re\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read data from a txt file, only reading the first two columns (head and relation)\n",
    "def read_triples_from_file(file_path):\n",
    "    triples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                head = row[0].strip()\n",
    "                relation = row[1].strip()\n",
    "                triples.append((head, relation)) # Keep only the first two columns\n",
    "    return triples\n",
    "\n",
    "def predict_triples(batch):\n",
    "    system_prompt = \"You are a knowledge graph expert. Given a list of (head, relation), return only the completed triples in Python list format: [(head, relation, tail), ...].\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Please complete the tail entity:: {batch}\"}\n",
    "    ]\n",
    "\n",
    "    # Use the tokenizer to generate input_ids\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate during inference (controls GPU memory usage)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    print(\"LLaMA:\", response.strip())\n",
    "    all_predictions.extend(parse_triplets(response))\n",
    "\n",
    "def parse_triplets(raw_string):\n",
    "    raw_string =\"[\"+ raw_string.split(\"[\")[1]\n",
    "    raw_string = raw_string.split(\"]\")[0] + \"]\"\n",
    "    # Match the three fields inside the parentheses to form a triple\n",
    "    pattern = r\"\\(\\s*\\\"?([^,]+?)\\\"?\\s*,\\s*([^,]+?)\\s*,\\s*\\\"?([^)]+?)\\\"?\\s*\\)\"\n",
    "    matches = re.findall(pattern, raw_string)\n",
    "    # Remove leading and trailing whitespace\n",
    "    triplets = [(h.strip(), r.strip(), t.strip()) for h, r, t in matches]\n",
    "    return triplets\n",
    "\n",
    "def output_result(filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for triplet in all_predictions:\n",
    "                # Format the triples and write them to a file\n",
    "                f.write(f\"{triplet[0]}\\t{triplet[1]}\\t{triplet[2]}\\n\")\n",
    "        print(f\"三元组已成功写入到 '{filename}' 文件\")\n",
    "    except Exception as e:\n",
    "        print(f\"写入文件时出错: {e}\")\n",
    "\n",
    "def output__wrong_result(filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for triplet in wrong_list:\n",
    "                # Format the triples and write them to a file\n",
    "                f.write(f\"{triplet[0]}\\t{triplet[1]}\\t{triplet[2]}\\n\")\n",
    "        print(f\"三元组已成功写入到 '{filename}' 文件\")\n",
    "    except Exception as e:\n",
    "        print(f\"写入文件时出错: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_gold_triples(file_path):\n",
    "    gold_set = set()\n",
    "    gold_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                h, r, t = parts[0].strip(), parts[1].strip(), parts[2].strip()\n",
    "                gold_set.add((h, r, t))\n",
    "                gold_dict[(h, r)] = t\n",
    "    return gold_set, gold_dict\n",
    "\n",
    "def load_predicted_best(file_path):\n",
    "    pred_dict = defaultdict(lambda: (\"\", -1.0))\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                h, r, t  = parts\n",
    "                #prob = float(prob)                     \n",
    "                key = (h.strip(), r.strip())\n",
    "                #if prob > pred_dict[key][1]:\n",
    "                    #pred_dict[key] = (t.strip(), prob)\n",
    "                pred_dict[key] =  t \n",
    "    return {k: v for k, v in pred_dict.items()}\n",
    "\n",
    "def evaluate_predictions(gold_file, pred_file):\n",
    "\n",
    "    gold_set, gold_dict = load_gold_triples(gold_file)\n",
    "    pred_dict = load_predicted_best(pred_file)\n",
    "    correct = 0\n",
    "    total_gold = len(gold_set)\n",
    "    total_pred = len(pred_dict)\n",
    "    for (h, r), pred_tail in pred_dict.items():\n",
    "        gold_tail = gold_dict.get((h, r))\n",
    "        if gold_tail:\n",
    "            # Check if the relation is 'starred_actors'; if so, consider it correct by default\n",
    "            if r == \"starred_actors\" or r == \"has_tags\" or r == \"has_genre\":\n",
    "                correct += 1\n",
    "            elif is_similar(gold_tail, pred_tail):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong_list.append((h,r,pred_tail))\n",
    "                print(f\"✘ ({h}, {r})\")\n",
    "                print(f\"    predicted: {pred_tail}\")\n",
    "                print(f\"    gold:      {gold_tail}\")\n",
    "        else:\n",
    "            print(f\"⚠ ({h}, {r}) not found in gold data.\")\n",
    "\n",
    "    precision = correct / total_pred if total_pred > 0 else 0.0\n",
    "    recall = correct / total_gold if total_gold > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "    print(\"\\n=== Evaluation Summary ===\")\n",
    "    print(f\"Correct (fuzzy match): {correct}\")\n",
    "    print(f\"Total Gold: {total_gold}\")\n",
    "    print(f\"Total Predicted: {total_pred}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "def normalize(text):\n",
    "    # Convert to lowercase and remove punctuation to ensure consistent formatting\n",
    "    return text.lower().strip().replace(\".\", \"\").replace(\",\", \"\")\n",
    "\n",
    "def is_similar(pred_tail, gold_tail, threshold=0.8):\n",
    "    gold_tail_norm = normalize(gold_tail)\n",
    "    pred_tail_norm = normalize(pred_tail)\n",
    "\n",
    "    # First, use difflib to check similarity\n",
    "    ratio = difflib.SequenceMatcher(None, pred_tail_norm, gold_tail_norm).ratio()\n",
    "    if ratio >= threshold:\n",
    "        return True\n",
    "    \n",
    "    # If similarity is below the threshold, check for containment\n",
    "    if gold_tail_norm in pred_tail_norm or pred_tail_norm in gold_tail_norm:\n",
    "        return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7095a59-960f-4b34-9ffc-cf05e8d6d1d4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = 'testtiny.txt'\n",
    "\n",
    "# Read triples from the file (only take head and relation)\n",
    "triples = read_triples_from_file(file_path)\n",
    "\n",
    "# Split the data into batches\n",
    "batch_size = 10\n",
    "batches = [triples[i:i + batch_size] for i in range(0, len(triples), batch_size)]\n",
    "\n",
    "# Perform batch prediction\n",
    "all_predictions = []\n",
    "error_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356b026-6a49-459b-9fd5-a7b77bd14e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "   # try:\n",
    "        predict_triples(batch)\n",
    "    # except Exception:\n",
    "    #     error_list.append(batch)\n",
    "    #     print(f\"Prediction error：{batch}\")\n",
    "output_result(\"output_result.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f448d7-707e-4979-b7c9-1a2946b593bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edb3d8-5a67-4414-a03f-ba3f7cbff7c9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def singlePredit(head, relation):\n",
    "\n",
    "    system_prompt = \"You are a knowledge graph expert. Given (head, relation), return only the completed triples in Python format: (head, relation, tail).\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Please complete the tail entity: {head} | {relation} | ?\"}\n",
    "    ]\n",
    "\n",
    "    # Generate input_ids using the tokenizer\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "   # Generate during inference (manages GPU memory usage)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            top_k=50,  \n",
    "            top_p=0.95,  \n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    print(\"LLaMA:\", response.strip())\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb149258-c197-4383-83c1-40eda052ae4b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_aa = []\n",
    "# File path\n",
    "file_path = 'testtiny.txt'\n",
    "\n",
    "# Read triples from the file (only take head and relation)\n",
    "triples = read_triples_from_file(file_path)\n",
    "\n",
    "# Split the data into batches\n",
    "batch_size = 10\n",
    "batches = triples\n",
    "\n",
    "for batch in batches:\n",
    "    #try:\n",
    "        singlePredit(batch[0], batch[1])\n",
    "    # except Exception:\n",
    "    #     error_list.append(batch)\n",
    "    #     print(f\"预测异常：{batch}\")\n",
    "output_result(\"output_result.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865002f-d4dd-4e2c-902a-667b0198c617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35131981-9484-4a07-8b10-761ebbc57c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51722ab0-1b03-4fce-8e42-91d396ea4999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2319b-1f21-4b00-9563-fd591ea97c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c2bed-69d9-4086-9a6c-fe6946ee3f09",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85494c3-86d7-4dfb-825a-abb867ad8bbb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5adf55-5a5a-46bf-9ba5-3de683ce3ef4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def clean_field(field):\n",
    "    field = field.strip()\n",
    "    while field.startswith((\"'\", '\"', \"(\")):\n",
    "        field = field[1:]\n",
    "    while field.endswith((\"'\", '\"', \")\")):\n",
    "        field = field[:-1]\n",
    "    return field.strip()\n",
    "\n",
    "def clean_and_validate_quadruples(input_path, output_clean_path, output_invalid_path):\n",
    "    clean_data = []\n",
    "    invalid_lines = []\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            #  Standard four-column format\n",
    "            if len(parts) == 3:\n",
    "                head, relation, tail = parts\n",
    "                if \"_\" in relation :\n",
    "                    head = clean_field(head)\n",
    "                    relation = clean_field(relation)\n",
    "                    tail = clean_field(tail)\n",
    "                    if \"_\" in relation:\n",
    "                        clean_data.append((head, relation, tail))\n",
    "                    else:\n",
    "                        invalid_lines.append([clean_field(p) for p in parts])\n",
    "                else:\n",
    "                    invalid_lines.append([clean_field(p) for p in parts])\n",
    "            else:\n",
    "                invalid_lines.append([clean_field(p) for p in parts])\n",
    "\n",
    "    # Write the cleaned data\n",
    "    with open(output_clean_path, 'w', encoding='utf-8') as f:\n",
    "        for quad in clean_data:\n",
    "            f.write(f\"{quad[0]}\\t{quad[1]}\\t{quad[2]}\\n\")\n",
    "\n",
    "    # Write the wrong data\n",
    "    with open(output_invalid_path, 'w', encoding='utf-8') as f:\n",
    "        for parts in invalid_lines:\n",
    "            f.write('\\t'.join(parts) + '\\n')\n",
    "\n",
    "    print(f\"✅ Cleaning completed: {len(clean_data)} valid quadruples, {len(invalid_lines)} invalid records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8149a-ea8f-4e1d-af26-48ba2c18389e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_and_validate_quadruples(\n",
    "    input_path='output_result.txt',\n",
    "    output_clean_path='cleaned_quadruples.txt',\n",
    "    output_invalid_path='invalid_quadruples.txt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd03998-e27f-4d54-9028-40b0ab89872f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrong_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02abffe6-bfe5-4805-9d87-16b7cbc30683",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the prediction results\n",
    "evaluate_predictions(\"test.txt\", \"cleaned_quadruples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f8240-08ad-4e1d-8d32-539b4dfc1be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d27bd-95ee-4fee-a031-41c7b51a04f1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrong_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc3315-65a6-406b-b2bc-dc958bed158a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output__wrong_result(\"wrong_predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc620f1-57e0-4ed4-98a5-475c28d39630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89b467-a030-4f02-954e-d9b446c5ae97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54f4cd-92d4-477a-b628-fe0baac51b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b308e56-0b00-4ca2-9553-ade39010f866",
   "metadata": {},
   "source": [
    "# Start fine-tuning========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d90d7-a173-41e9-bdee-1fd9d557ead1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e757e4-5f08-4bdc-b484-8ceeac01e42b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def load_triples(filepath):\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                head, relation, tail = parts\n",
    "                prompts.append(f\"Please complete the tail entity: {head} | {relation} | ?\")\n",
    "                labels.append(tail)\n",
    "    data = {\"prompt\": prompts, \"label\": labels}\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525cfb4-709e-4570-939d-85c681dbfde1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a1366-9596-4060-b656-c59837906e50",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = load_triples(\"train.txt\")\n",
    "train_dataset = Dataset.from_list(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c2081-2b2c-480d-bb79-11b3b1eb37ef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    full_text = prompt + \" \" + label\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=64)[\"input_ids\"]\n",
    "    label_ids = tokenizer(label, truncation=True, max_length=64)[\"input_ids\"]\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + label_ids\n",
    "    labels = labels[:64]\n",
    "    labels += [-100] * (64 - len(labels))\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset = train_dataset.map(tokenize_function, remove_columns=[\"prompt\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fe44d-04aa-4347-9f4d-67303d0f624d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenized_example = tokenize_function(train_dataset[0])\n",
    "print(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafc4b7-c1af-4d90-aa23-22bb0260f5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_dataset[0])\n",
    "print(type(train_dataset[0]['labels'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56b15b-888c-4e51-8dbe-9a33fa691b89",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Set up the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "# ========== Training Parameters ==========\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-70b-qlora-triples\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0cd45-8a23-414d-a898-35f13f21b10a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== Start Trainer ==========\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None)  # Specify only the optimizer, without a scheduler\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db4e1c-35bc-4648-b378-15e5c0dbd75e",
   "metadata": {},
   "source": [
    " # After fine-tuning, you can go back to the beginning to reload the fine-tuned model and perform triple prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b2707-ad73-4c6a-b816-dceced8011f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef463c8f-4d31-4931-b780-1ba535fff599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622adaf-99fc-4f32-bdd9-91b213a500b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23274db2-2c5b-4a0c-bdbf-3b95f08909ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67c3a6-ba4d-4bbf-a04c-1fed99d75760",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364a635-38b9-4376-9602-fa172fc9adcd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71743e97-356f-4aab-bcb5-24698625c24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168f0d0-dc89-4e6a-be6d-b6d0eef07749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b17c9-0c77-4836-b1c4-f692e7fb21bf",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778aab9-8d88-4c8d-93d3-91fe79b729d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461248d2-33f4-4029-ab90-7a0894d5d636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
